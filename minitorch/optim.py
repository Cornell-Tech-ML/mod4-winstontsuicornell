from typing import Sequence

from .module import Parameter
from .scalar import Scalar


class Optimizer:
    def __init__(self, parameters: Sequence[Parameter]):
        self.parameters = parameters


class SGD(Optimizer):
    def __init__(self, parameters: Sequence[Parameter], lr: float = 1.0):
        super().__init__(parameters)
        self.lr = lr

    def zero_grad(self) -> None:
        """Resets the gradients of all parameters to zero.

        This method iterates through all the parameters associated with the object
        and sets their gradients and derivatives to None, effectively clearing any
        previously computed gradients. It ensures that no residual gradients are
        carried over to the next training step.

        """
        for p in self.parameters:
            if p.value is None:
                continue
            if hasattr(p.value, "derivative"):
                if p.value.derivative is not None:
                    p.value.derivative = None
            if hasattr(p.value, "grad"):
                if p.value.grad is not None:
                    p.value.grad = None

    def step(self) -> None:
        """Updates all parameters using their gradients or derivatives and the learning rate.

        This method iterates through all the parameters and updates their values using
        their computed gradients or derivatives, scaled by the learning rate (`self.lr`).
        If a parameter has a `derivative` attribute, the parameter is updated using this
        value. Otherwise, if it has a `grad` attribute, it uses the gradient for the update.
        """
        for p in self.parameters:
            if p.value is None:
                continue
            if hasattr(p.value, "derivative"):
                if p.value.derivative is not None:
                    p.update(Scalar(p.value.data - self.lr * p.value.derivative))
            elif hasattr(p.value, "grad"):
                if p.value.grad is not None:
                    p.update(p.value - self.lr * p.value.grad)
